Metadata-Version: 2.4
Name: chemlearn-labos
Version: 0.1.0
Summary: ChemLearn LabOS core scaffolding
Author-email: ChemLearn Swarm <labs@chemlearn.example>
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.10
Description-Content-Type: text/markdown

# ChemLearn LabOS

ChemLearn LabOS is a faith-aligned lab operating system that coordinates experiments, jobs, datasets, and scientific learning tools across a unified stack. The repository is currently in **Phase 0 (Bootstrap)**, which means only scaffolding, governance docs, and placeholders are being createdâ€”no production logic exists yet.

## Three-Layer Architecture
- **LabOS Core:** Orchestrates experiments, jobs, datasets, audit logging, and provenance workflows.
- **Scientific Modules:** Plug-in domain packs (PChem, EI-MS, proteomics, etc.) that extend the core with discipline-specific capabilities.
- **UI Layer:** Control panel and presentation shells for learners, lab operators, and builders.

## Phase 1 Kickoff
- Adds the first cut of the LabOS Core Python package with configuration loading, audit logging, and JSON-backed registries.
- Introduces a minimal CLI (`labos`) for initializing directories, creating experiments, registering datasets, and dispatching module jobs.
- Keeps scientific logic out of the core to remain compliant until validation dossiers are ready.

## Getting Started
1. Create a virtual environment and install the package in editable mode:
	```powershell
	python -m venv .venv
	.\.venv\Scripts\activate
	pip install -e .
	```
2. Initialize the workspace directories and data stores:
	```powershell
	labos init
	```
3. Create an experiment and register datasets as needed:
	```powershell
	labos new-experiment --user student --title "Week1" --purpose "Buffer prep"
	labos register-dataset --owner student --dataset-type experimental --uri s3://placeholder
	```
4. Wire scientific modules (when available) via `LABOS_MODULES` and execute them:
	```powershell
	labos run-module --experiment-id <id> --module-id demo --operation echo --actor student --params-json '{"value": 42}'
	```

## Roadmap Snapshot
1. Harden experiment/job/dataset registries with persistence and validation evidence.
2. Define structured data stores and provenance tracking under `data/` per `DATA_ARCHITECTURE.md`.
3. Prototype the control panel shell and connect initial scientific modules.

## Binary Assets
Large artifacts exported from CODEX or similar tools must be tracked through Git Large File Storage (LFS). Install Git LFS locally, keep binaries under `artifacts/` or `datasets/`, and follow `docs/BINARY_ASSET_HANDLING.md` to stay within GitHub limits.
